{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tools import plot_results, run_experiment, BasicModel, get_flops_and_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvFlops(nn.Conv2d):\n",
    "\n",
    "    \"\"\"\n",
    "    Flops are computed for square kernel\n",
    "    FLOPs = 2 x Cin x Cout x k**2 x Wout x Hout / groups\n",
    "    We use 2 because 1 for multiplocation and 1 for addition\n",
    "    Hout = Hin + 2*padding[0] - dilation[0] x (kernel[0]-1)-1\n",
    "          --------------------------------------------------- + 1\n",
    "                                stride\n",
    "    Wout same as above\n",
    "    NOTE: We do not account for bias term\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvFlops, self).__init__(**kwargs)\n",
    "        self.kernel = self.to_tuple(self.kernel_size)\n",
    "\n",
    "        self.param_size = (\n",
    "            2\n",
    "            * self.in_channels\n",
    "            * self.out_channels\n",
    "            * self.kernel[0]\n",
    "            * self.kernel[1]\n",
    "            / self.groups\n",
    "        )\n",
    "        self.flops = 0\n",
    "        self.memory_size = 0\n",
    "\n",
    "    def to_tuple(self, value):\n",
    "        if type(value) == int:\n",
    "            return (value, value)\n",
    "        if type(value) == tuple:\n",
    "            return value\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        \"\"\"\n",
    "        BATCH x C x W x H\n",
    "        \"\"\"\n",
    "        # get the same device to avoid errors\n",
    "        output = self._conv_forward(input_x, self.weight, bias=None)\n",
    "\n",
    "\n",
    "        # WRITE A FEW LINES OF CODE TO COMPUTE FLOPS AND MEMORY USAGE\n",
    "\n",
    "        self.memory_size = \n",
    "        self.flops = \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _fetch_info(self):\n",
    "        return self.b*self.flops, self.memory_size\n",
    "\n",
    "\n",
    "class LinearFlops(nn.Linear):\n",
    "\n",
    "    \"\"\"\n",
    "    V in R^(m)\n",
    "    W in R^(mxn)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LinearFlops, self).__init__(**kwargs)\n",
    "\n",
    "        self.flops = 0\n",
    "        self.memory_size = 0\n",
    "\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        \"\"\"\n",
    "        BATCH x m\n",
    "        \"\"\"\n",
    "\n",
    "        output = torch.nn.functional.linear(input_x, self.weight, bias=self.bias)\n",
    "        \n",
    "        self.b = output.shape[0]\n",
    "\n",
    "        # WRITE A FEW LINES OF CODE TO COMPUTE FLOPS AND MEMORY USAGE\n",
    "\n",
    "        self.memory_size =\n",
    "        self.flops = \n",
    "\n",
    "        return output\n",
    "\n",
    "    def _fetch_info(self):\n",
    "        # get flops according to the batch size\n",
    "        return self.b*self.flops, self.memory_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if it is correctly computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ConvFlops(in_channels=15,out_channels=3,kernel_size=2)\n",
    "\n",
    "tensor = torch.rand(7,15,23,21)\n",
    "conv(tensor)\n",
    "\n",
    "flops, memory = conv._fetch_info()\n",
    "\n",
    "assert flops == 1108800.0, \"wrong flops numebr\"\n",
    "assert memory == 7245, \"wrong memory size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_in: int,\n",
    "        features_out: int,\n",
    "        layer_type: str = 'linear',\n",
    "        kernel_size: int = 3,\n",
    "        dilation: int = 1\n",
    "        )-> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert layer_type in ['linear','conv'], \"layer_type should either linear or conv\"\n",
    "\n",
    "        match layer_type:\n",
    "            case 'linear':\n",
    "                self.l1 =  LinearFlops(in_features=features_in,out_features=features_out, bias=1)\n",
    "                self.l2 = LinearFlops(in_features=features_in,out_features=features_out, bias=1)\n",
    "                norm_layer = nn.BatchNorm1d\n",
    "\n",
    "            case 'conv':\n",
    "                self.l1 =  ConvFlops(in_channels=features_in,out_channels=features_out,kernel_size=kernel_size,  padding=1)\n",
    "                self.l2 = ConvFlops(in_channels=features_in,out_channels=features_out,kernel_size=kernel_size,  padding=1)\n",
    "                norm_layer = nn.BatchNorm2d\n",
    "\n",
    "\n",
    "        self.bn1 = norm_layer(features_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn2 = norm_layer(features_out)\n",
    "\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = x\n",
    "\n",
    "        out = self.l1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.l2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments :\n",
    "\n",
    "We wanto investigate how lattency scales with memory size and used memmory\n",
    "\n",
    "We will run several experiments:\n",
    "\n",
    "1. Vary an input size\n",
    "2. Vary a number of blocks\n",
    "3. Measure performance for both, linear models, convolutional models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that everything works as intented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_time': 1.4360620766878127,\n",
       " 'std_time': 0.07505615384999156,\n",
       " 'flops': 58982400.0,\n",
       " 'memory': 51200}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_tensor = torch.rand(1,64,20,20)\n",
    "\n",
    "run_experiment(BasicBlock,\n",
    "               layer_type='conv', \n",
    "                   features_in=64, \n",
    "                   features_out=64, \n",
    "                   dummy_tensor=dummy_tensor, \n",
    "                   block_numbers=1, \n",
    "                   device=1, \n",
    "                   n_runs = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear layer varied input size:\n",
    "\n",
    "linear_sizes = [10,100, 500, 1000, 5000, 10000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:18<00:00, 23.14s/it]\n"
     ]
    }
   ],
   "source": [
    "linear_sizes = [10,100, 500, 1000, 5000, 10000] \n",
    "\n",
    "results = {'mean_time':[], 'std_time':[],  'flops':[],'memory':[]}\n",
    "\n",
    "for sisze in tqdm(linear_sizes):\n",
    "\n",
    "    dummy_tensor = torch.rand(1,sisze,sisze)\n",
    "\n",
    "    r = run_experiment(BasicBlock,\n",
    "                       layer_type='linear', \n",
    "                    features_in=sisze, \n",
    "                    features_out=sisze, \n",
    "                    dummy_tensor=dummy_tensor, \n",
    "                    block_numbers=1, \n",
    "                    device=1, \n",
    "                    n_runs = 500)\n",
    "\n",
    "    for k in results: results[k].append(r[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results, name='Linear model with varied input size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear layer with varied number of layers GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bocks = [5,10,20, 30, 40, 50] \n",
    "\n",
    "results = {'mean_time':[], 'std_time':[],  'flops':[],'memory':[]}\n",
    "\n",
    "for b_sisze in tqdm(linear_sizes):\n",
    "\n",
    "    dummy_tensor = torch.rand(1,100,100)\n",
    "\n",
    "    r = run_experiment(BasicBlock,\n",
    "                       layer_type='linear', \n",
    "                    features_in=100, \n",
    "                    features_out=100, \n",
    "                    dummy_tensor=dummy_tensor, \n",
    "                    block_numbers=b_sisze, \n",
    "                    device=1, \n",
    "                    n_runs = 100)\n",
    "\n",
    "    for k in results: results[k].append(r[k])\n",
    "\n",
    "\n",
    "plot_results(results, name='Linear layer with varied number of layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv layer with varied number of channels 100x100 image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_sizes = [32,64, 128, 256, 512, 1024] \n",
    "\n",
    "results = {'mean_time':[], 'std_time':[],  'flops':[],'memory':[]}\n",
    "\n",
    "for sisze in tqdm(linear_sizes):\n",
    "\n",
    "    dummy_tensor = torch.rand(1,sisze,100,100)\n",
    "\n",
    "    r = run_experiment(BasicBlock,\n",
    "                       layer_type='conv', \n",
    "                    features_in=sisze, \n",
    "                    features_out=sisze, \n",
    "                    dummy_tensor=dummy_tensor, \n",
    "                    block_numbers=1, \n",
    "                    device=1, \n",
    "                    n_runs = 500)\n",
    "\n",
    "    for k in results: results[k].append(r[k])\n",
    "\n",
    "\n",
    "plot_results(results, name='Conv layer with varied number of channels 100x100 image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv layer with varied number of channels 20x20 image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_sizes = [32,64, 128, 256, 512, 1024] \n",
    "\n",
    "results = {'mean_time':[], 'std_time':[],  'flops':[],'memory':[]}\n",
    "\n",
    "for sisze in tqdm(linear_sizes):\n",
    "\n",
    "    dummy_tensor = torch.rand(1,sisze,20,20)\n",
    "\n",
    "    r = run_experiment(BasicBlock,\n",
    "                       layer_type='conv', \n",
    "                    features_in=sisze, \n",
    "                    features_out=sisze, \n",
    "                    dummy_tensor=dummy_tensor, \n",
    "                    block_numbers=1, \n",
    "                    device=1, \n",
    "                    n_runs = 500)\n",
    "\n",
    "    for k in results: results[k].append(r[k])\n",
    "\n",
    "\n",
    "plot_results(results, name='Conv layer with varied number of channels 20x20 image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv layer with varied number of layers 400x400 image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [4,8,16, 32, 64, 128] \n",
    "\n",
    "results = {'mean_time':[], 'std_time':[],  'flops':[],'memory':[]}\n",
    "\n",
    "for sisze in tqdm(linear_sizes):\n",
    "\n",
    "    dummy_tensor = torch.rand(1,sisze,400,400)\n",
    "\n",
    "    r = run_experiment(BasicBlock,\n",
    "                       layer_type='conv', \n",
    "                    features_in=sisze, \n",
    "                    features_out=sisze, \n",
    "                    dummy_tensor=dummy_tensor, \n",
    "                    block_numbers=1, \n",
    "                    device=1, \n",
    "                    n_runs = 500)\n",
    "\n",
    "    for k in results: results[k].append(r[k])\n",
    "\n",
    "\n",
    "plot_results(results, name='Conv layer with varied number of layers 400x400 image')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
